{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project our goal is to extract the facial regions using the the facial landmark coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will extract the facial regions. We will make a dictionary to access these facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps the indexes of the facial\n",
    "# landmarks to specific face regions\n",
    "FACIAL_LANDMARKS_IDXS = ([\n",
    "    (\"mouth\", (48, 68)),\n",
    "    (\"right_eyebrow\", (17, 22)),\n",
    "    (\"left_eyebrow\", (22, 27)),\n",
    "    (\"right_eye\", (36, 42)),\n",
    "    (\"left_eye\", (42, 48)),\n",
    "    (\"nose\", (27, 35)),\n",
    "    (\"jaw\", (0, 17))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will make a function to visualize the different facial landmarks. We will have four arguments (image, shape, colors, alpha) in which two are optional (colors, alpha) First we create two copies of input image so that we can draw the semi overlay over the different regions of face. Then we will check if there is any color or not. Then we will loop over each of the facial landmarks from the FACIAL_LANDMARKS_IDXS dictionary. We will check to see if we are drawing on jaws. If so, we loop over each individual points of jaws and draw the lines over the jaws. Then the last step will be to draw the overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def visualize_facial_landmarks(image, shape, colors=None, alpha=0.75):\n",
    "    # create two copies of the input image -- one for the\n",
    "    # overlay and one for the final output image\n",
    "    overlay = image.copy()\n",
    "    output = image.copy()\n",
    "    # if the colors list is None, initialize it with a unique\n",
    "    # color for each facial landmark region\n",
    "    if colors is None:\n",
    "        colors = [(19, 199, 109), (79, 76, 240), (230, 159, 23),\n",
    "            (168, 100, 168), (158, 163, 32),\n",
    "            (163, 38, 32), (180, 42, 220)]\n",
    "        \n",
    "    # loop over the facial landmark regions individually\n",
    "    for (i, name) in enumerate(FACIAL_LANDMARKS_IDXS.keys()):\n",
    "        # grab the (x, y)-coordinates associated with the\n",
    "        # face landmark\n",
    "        (j, k) = FACIAL_LANDMARKS_IDXS[name]\n",
    "        pts = shape[j:k]\n",
    "        # check if are supposed to draw the jawline\n",
    "        if name == \"jaw\":\n",
    "            # since the jawline is a non-enclosed facial region,\n",
    "            # just draw lines between the (x, y)-coordinates\n",
    "            for l in range(1, len(pts)):\n",
    "                ptA = tuple(pts[l - 1])\n",
    "                ptB = tuple(pts[l])\n",
    "                cv2.line(overlay, ptA, ptB, colors[i], 2)\n",
    "        # otherwise, compute the convex hull of the facial\n",
    "        # landmark coordinates points and display it\n",
    "        else:\n",
    "            hull = cv2.convexHull(pts)\n",
    "            cv2.drawContours(overlay, [hull], -1, colors[i], -1)\n",
    "            \n",
    "    # apply the transparent overlay\n",
    "    cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "    # return the output image\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load instatiate our detector and upload the \"shape_predictor_68_face_landmarks.dat\" predictor. Then we upload our image, resize it and convert it grayscale. Then we detect the faces on our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# initialize the dlibs face detector and then\n",
    "# create the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\".\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# load the input image,resize it and convert it to grayscale\n",
    "image = cv2.imread(\".\\images\\example_2.jpg\")\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect the faces in grayscale\n",
    "rects = detector(gray, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the individual ROI. We determine the facial landmarks and convert them to numpy arrays. Then for each face parts, we loop pver them individually and draw name of the faces and draw the circles over the facial landmark points. We them compute the bounding associated with specific region and use numpy arrray slicing to extract them. Then we finally display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, the\n",
    "    # convert the landmarks coordinates to numpy array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "    # loop over the face parts individually\n",
    "    for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
    "        # clone the original image so we can draw on it, then\n",
    "        # display the name of the face part on the image\n",
    "        clone = image.copy()\n",
    "        cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # loop over the subset of facial landmarks, drawing the\n",
    "        # specific face part\n",
    "        for (x, y) in shape[i:j]:\n",
    "            cv2.circle(clone, (x, y), 1, (0, 255, 0), -1)\n",
    "            \n",
    "        # extract the ROI of the face region as saperate image\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "        roi = image[y:y + h, x: x + w]\n",
    "        roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # show the particular face regions\n",
    "        cv2.imshow(\"ROI\", roi)\n",
    "        cv2.imshow(\"Image\", clone)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "    # visualize the all the facial landmarks with transparent overlay\n",
    "    output = face_utils.visualize_facial_landmarks(image, shape)\n",
    "    cv2.imshow(\"Image\", output)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
